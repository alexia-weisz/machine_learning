Week 1
------

Machine learning is the science of getting computers to learn without being explicitly programmed. (Arthur Samuel, 1959)

**INTRODUCTION**
Grew out of work in AI -- new capabilities for computers

Good for database mining -- industry, medicine, biology, engineering, etc; many examples of where machine learning allows us to take large amounts of data and use it

good for applications that we can't program by hand -- e.g., how do you write a program to make a helicopter fly by itself?

good for self-customizing programs -- amazon, netflix reccomendations

good for understanding how humans learn --  brain, real AI

A computer is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. (Tom Mitchell, 1998)

Supervised learning -- we teach the computer about something
Unsupervised learning -- the computer learns for itself

Supervised learning -- we know the right answer and need to figure out how to teach the computer how to get that answer with the available data.
-- Regression -- continuous output
-- Classificiation -- discrete output


Supervised Learning -- Model Representation
Training Set -- Learning Algorithm -- hypothesis (h) -- takes in x, outputs estimated value of y; i.e., h maps from x's to y's

How do we represent h?
 -- A straight line function -- simplest case (Linear Regression with 1 variable, or univariate linear regression
   minimize cost function (squared error function)
   why do we take the square of the errors?
 
Gradient decent: start with guess, change inital guess values by a little bit each time in order to find the values that minimize the function

:= == assignment operator (= in python)
= == truth assertion (== in python)

Linear regression cost function is always a convex function, so always get convergence to global minimum because there are no local minima




Week 2
------
Installing Octave
To uninstall, move following folders to trash
  -- /usr/local/octave/3.8.0
  -- /Applications/Octave-cii.app
  -- /Applications/Octive-gui.app


Linear Regression with Multiple Variables
-- convention: x_0 = 1

Gradient Descent:
-- Feature Scaling: want to get every feature into approximately a -1 < x_i < 1 range. Keep the scales of the features similar to help gradient descent converge faster. Also want to try to rescale such that the average value is 0, so could be x_i = (xi_i - mu_i)/s_i), where mu_i is average value of trianing set, and s_i is the range of the data set (i.e., max-min, or std)
-- make sure it's working correctly by plotting the cost function J(theta) as a function of number of iterations. The cost function should decrease after each iteration if gradient descent is working.
-- Hard to know in advance how many iterations you need. Plotting can help.
-- Can also declare convergence in advance if J(theta) decreases by less than a set amount in one iteration. But it can be difficult to know waht that amount should be.
-- If gradient descent is not working (i.e., cost function is increasing with iterations), could be that learning rate alpha is too large, so try a smaller value. For sufficiently small alpha, J(theta) should decrease on every iteration.

Normal Equation: analytically solve for your vector theta
Can be used if the number of features is not too large (i.e., n<~10000)
Alternative that can be much faster than gradient descent

why might X-transpose * x be non-inverible?
- redundent features (i.e., x1 and x2 are related, linearly dependent)
- too many features (i.e., the number of examples in your training set is less than the number of features)




Week 3
-------
Not a good idea to use linear regression for a classification problem. Can work sometimes, but outliers can really affect the results.
Logistic regression — output always between 0 and 1 — is a classification algorithm

Logistic Regression — Hypothesis Representation
— want model to output values between 0 and 1
— sigmoid function or logistic function — asymptotes at 0 and one, classes y-axis at 0.5

Logistic Regression — Decision Boundary

Logistic Regression — Cost Function — Cost the algorithm pays
— want a convex cost function so that gradient descent converges to global minimum
— can’t use previously used square cost function

Cost(h(x),y) = -y log(h(x)) - (1 - y) log(1 - h(x))
y can only be 0 or 1
Finally, to find the parameters theta, we need to minimize the cost function J(theta)
Gradient descent rule for logistic regression is same as that for linear regression, but the definition for the hypothesis h(x) has changed.

Multiclass classification — one-vs-all (i.e., classification with more than two classes)
 — divide into n binary classifiers for n classes
 — pick single classifier that maximizes the probability

Overfitting problem: too many features, you can fit the training set well, but cannot be generalized to new examples. Fit training set too well.
—underfit: ‘high bias’
-overfit: ‘high variance’

How do you address overfitting?
— could plot the hypothesis to see that the problem is overfit — doesn’t always work, can be hard when there are many features
Options:
1. Reduce number of features — manually, or with a model selection algorithm
2. Regularization — keep all features, but reduce magnitude of parameters theta

Regularization:
— small parameters leads to a simpler hypothesis, which is less prone to overfitting
— be careful about choice of regularization parameter lambda: too large can underfit the data

